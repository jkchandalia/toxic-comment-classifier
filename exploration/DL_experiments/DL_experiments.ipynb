{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About this Notebook\n",
    "\n",
    "The goal of this notebook is to build a DL classifier to find toxic comments. The data has been taken from a series of Kaggle competitions to classify Wikipedia comments as toxic/nontoxic. The data has been sourced from Google and Jigsaw. \n",
    "\n",
    "Though the full dataset includes non-English comments, I will restrict myself to English-only comment for this iteration. \n",
    "\n",
    "I will explore deep learning approaches, using a combination of pretrained word embeddings and simple deep learning models like RNNs and 1D convolutions to do more benchmarking. \n",
    "\n",
    "Next, we will explore deep learning models that have 'memory' using LSTMs (Long Short Term Memory) and GRUs (Gated Recurrent Units). \n",
    "\n",
    "Finally, we will approach state of the art performance using pretrained models like BERT and xlnet.\n",
    "\n",
    "For metrics, I will focus on both ROC and precision-recall curves. In addition, I will look at the confusion matrix and performance across different flavors of toxicity.\n",
    "\n",
    "Credits:\n",
    "- https://www.kaggle.com/tanulsingh077/deep-learning-for-nlp-zero-to-transformers-bert\n",
    "- https://www.kaggle.com/jagangupta/stop-the-s-toxic-comments-eda\n",
    "- https://www.kaggle.com/clinma/eda-toxic-comment-classification-challenge\n",
    "- https://www.kaggle.com/abhi111/naive-bayes-baseline-and-logistic-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My approach to feature engineering and building the model is below:\n",
    "\n",
    "Deep Learning:\n",
    "1. Use standard tokenizers and compare with 'homegrown' version from above.\n",
    "2. Use open source word embeddings for corpus as input to RNN models. Quantify how misspellings affect the standard tokenizers.\n",
    "3. Find way to input additional features like punctuation/capitalization from approach above to Deep Learning RNN models.\n",
    "4. Try progressively more complicated deep learning sequence models approaching SOTA.\n",
    "5. Use metrics from above.\n",
    "\n",
    "Potential Modules:\n",
    "1. Correct misspellings\n",
    "2. Analytics for preprocessing\n",
    "3. Analytics for model performance (use multi-labels, make easy way to look at specific examples)\n",
    "4. Automatically generate a lookup table for common variations of words (particularly toxic words, e.g., 'mothafucka' -> 'motherfucker')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from collections import defaultdict as ddict, Counter\n",
    "from itertools import compress\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from plotly import graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import re\n",
    "\n",
    "import random\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Tweet tokenizer does not split at apostophes which is what we want\n",
    "from nltk.tokenize import TweetTokenizer   \n",
    "pd.options.display.max_rows = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/jkc/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jkc/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/jkc/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jkc/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/jkc/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jkc/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from toxicity import constants, data, features, text_preprocessing, model, metrics, visualize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_path = './../'+constants.INPUT_PATH\n",
    "df_train = data.load(input_data_path, filter=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_metrics(predictions, predictions_prob, target, visualize=True):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(target, predictions_prob)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(target, predictions_prob)\n",
    "    average_precision = metrics.average_precision_score(yvalid, pred)\n",
    "    #average_recall = metrics.recall_score(yvalid, pred)\n",
    "    print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision))\n",
    "    accuracy = metrics.accuracy_score(yvalid, pred)\n",
    "    print(metrics.confusion_matrix(yvalid, pred, labels=[0,1]))\n",
    "    print(\"Accuracy Score: {0:0.2f}\".format(accuracy))\n",
    "    if visualize:\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr)\n",
    "        plt.title('ROC curve, AUC: {0:0.2f}'.format(roc_auc))\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(recall, precision)\n",
    "        plt.title('Precision-Recall curve')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.show()\n",
    "        #disp = metrics.plot_precision_recall_curve(nb_classifier, count_valid, yvalid)\n",
    "        #disp.ax_.set_title('2-class Precision-Recall curve: '\n",
    "                   #'AP={0:0.2f}'.format(average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xvalid, ytrain, yvalid = model.make_train_test(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44710"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xvalid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU,SimpleRNN\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping, History, ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will check the maximum number of words that can be present in a comment , this will help us in padding later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of comment text is: 2400\n"
     ]
    }
   ],
   "source": [
    "max_len = int(round(df_train['comment_text'].apply(lambda x:len(str(x).split())).max(), -2)+100)\n",
    "print(\"Max length of comment text is: {}\".format(max_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First do Tokenization of input corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using keras tokenizer here\n",
    "token = text.Tokenizer(num_words=None)\n",
    "token_toxic = text.Tokenizer(num_words=None)\n",
    "token_nontoxic = text.Tokenizer(num_words=None)\n",
    "\n",
    "token.fit_on_texts(list(xtrain) + list(xvalid))\n",
    "token_toxic.fit_on_texts(train.comment_text.values[train.toxic==1])\n",
    "token_nontoxic.fit_on_texts(train.comment_text.values[train.toxic==0])\n",
    "\n",
    "xtrain_seq = token.texts_to_sequences(xtrain)\n",
    "xvalid_seq = token.texts_to_sequences(xvalid)\n",
    "\n",
    "#zero pad the sequences\n",
    "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_toxic = token_toxic.word_index\n",
    "word_nontoxic = token_nontoxic.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(word_toxic))\n",
    "print(len(word_nontoxic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example for fitting tokenizer line-by-line if corpus is too big to fit into memory\n",
    "\n",
    "with open('/Users/liling.tan/test.txt') as fin: for line in fin:\n",
    "t.fit_on_texts(line.split()) # Fitting the tokenizer line-by-line.\n",
    "\n",
    "M = []\n",
    "\n",
    "with open('/Users/liling.tan/test.txt') as fin: for line in fin:\n",
    "\n",
    "    # Converting the lines into matrix, line-by-line.\n",
    "    m = t.texts_to_matrix([line], mode='count')[0]\n",
    "    M.append(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use pretrained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert our one-hot word index into semantic rich GloVe vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the GloVe vectors in a dictionary:\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(pre_path + 'glove840b300dtxt/glove.840B.300d.txt','r',encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.split(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray([float(val) for val in values[1:]])\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "words_not_in_corpus = ddict(int)\n",
    "words_in_corpus = ddict(int)\n",
    "# create an embedding matrix for the words we have in the dataset\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in tqdm(word_nontoxic.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        words_in_corpus[word]+=1\n",
    "    else:\n",
    "        words_not_in_corpus[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(words_not_in_corpus))\n",
    "print(len(words_in_corpus))\n",
    "max(words_not_in_corpus.values())\n",
    "max(words_in_corpus.values())\n",
    "\n",
    "#For the full dataset, more than half the 'words' are not found in the glove embeddings\n",
    "#For the 10K sample dataset, only ~25% of the words are not found in the glove embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(words_not_in_corpus))\n",
    "print(len(words_in_corpus))\n",
    "max(words_not_in_corpus.values())\n",
    "max(words_in_corpus.values())\n",
    "\n",
    "#For the full dataset, more than half the 'words' are not found in the glove embeddings\n",
    "#For the 10K sample dataset, only ~25% of the words are not found in the glove embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save embeddings so they can be easily loaded\n",
    "np.save('/kaggle/working/glove_embedding_for_full_data', embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load embeddings\n",
    "embedding_matrix = np.load('/kaggle/working/glove_embedding_for_10K_sample.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Embedding(len(word_index) + 1,\n",
    "                 300,\n",
    "                 input_length=max_len))\n",
    "model1.add(SimpleRNN(100))\n",
    "model1.add(Dense(1, activation='relu'))\n",
    "model1.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "EPOCHS = 10\n",
    "checkpoint_filepath = '/kaggle/working/'\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_acc',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "\n",
    "my_callbacks = [\n",
    "    model_checkpoint_callback,\n",
    "    TensorBoard(log_dir='/kaggle/working/logs'),\n",
    "    EarlyStopping(monitor='val_loss', patience=3)\n",
    "]\n",
    "model_checkpoint_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.fit(xtrain_pad, \n",
    "           ytrain, \n",
    "           epochs=50, \n",
    "           batch_size=100, \n",
    "           callbacks=my_callbacks,\n",
    "           validation_split=0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model1.predict(xvalid_pad)[:, 0]\n",
    "preds = scores>.5\n",
    "run_metrics(preds, scores, yvalid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# A simple LSTM with glove embeddings and one dense layer\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                 300,\n",
    "                 weights=[embedding_matrix],\n",
    "                 input_length=max_len,\n",
    "                 trainable=False))\n",
    "\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(xtrain_pad, \n",
    "          ytrain, \n",
    "          epochs=50, \n",
    "          batch_size=100,\n",
    "          callbacks=my_callbacks,\n",
    "          validation_split=0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.predict(xvalid_pad)\n",
    "preds = scores>.5\n",
    "run_metrics(preds, scores, yvalid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, with very little preprocessing, we have achieved high accuracy. This is a little bit misleading however because the training set is highly imbalanced (roughly 10% positive/toxic class). \n",
    "\n",
    "Slightly older techniques, bag-of-words and tf-idf have done better than a simple deep learning models out-of-the-box. This can been seen by the higher AUCs and accuracy of these models in contrast to the simple RNN model. In addition, training these models was extremely fast, even on a local machine. In contrast, the deep learning models required more than 10 minutes to train even five epochs. In addition, trainingg the simple RNN required playing around with the learning rate to get network to learn. The first few attempts produced labels of all zeros. \n",
    "\n",
    "The simple LSTM model starts to improve dramatically over the simple RNN model even with only 5 epochs, showing that using the semantic rich word embeddings and including memory already improve simple deep learning results. Though the overall accuracy has decreased in the LSTM model vs the Naive Bayes models, the AUC and precision-recall and ROC curves are much better than the simple models. As we approach more state-of-the-art (SOTA) models and move beyond simple proof-of-concept model training, i.e., try different network parameters, experiment with data preprocessing, do hyperparameter optimization, train until the results start to degrade, add regularization, etc., the results will likely improve even more dramatically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try a GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# GRU with glove embeddings and two dense layers\n",
    " model = Sequential()\n",
    " model.add(Embedding(len(word_index) + 1,\n",
    "                 300,\n",
    "                 weights=[embedding_matrix],\n",
    "                 input_length=max_len,\n",
    "                 trainable=False))\n",
    " model.add(SpatialDropout1D(0.3))\n",
    " model.add(GRU(300))\n",
    " model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    " model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])   \n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# GRU with glove embeddings and two dense layers\n",
    " model = Sequential()\n",
    " model.add(Embedding(len(word_index) + 1,\n",
    "                 300,\n",
    "                 weights=[embedding_matrix],\n",
    "                 input_length=max_len,\n",
    "                 trainable=False))\n",
    " model.add(SpatialDropout1D(0.3))\n",
    " model.add(GRU(300))\n",
    " model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    " model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])   \n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(xtrain_pad, ytrain, nb_epoch=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.predict(xvalid_pad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# A simple bidirectional LSTM with glove embeddings and one dense layer\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                 300,\n",
    "                 weights=[embedding_matrix],\n",
    "                 input_length=max_len,\n",
    "                 trainable=False))\n",
    "model.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n",
    "\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(xtrain_pad, ytrain, nb_epoch=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.predict(xvalid_pad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2seq Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TBD\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
