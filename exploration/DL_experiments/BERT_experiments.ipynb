{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About this Notebook\n",
    "\n",
    "The goal of this notebook is to build a classifier using a BERT pre-trained model to find toxic comments. The data has been taken from a series of Kaggle competitions to classify Wikipedia comments as toxic/nontoxic. The data has been sourced from Google and Jigsaw. \n",
    "\n",
    "Though the full dataset includes non-English comments, I will restrict myself to English-only comment for this iteration. \n",
    "\n",
    "For metrics, I will focus on both AUC for ROC and precision-recall curves. In addition, I will look at overall accuracy and perhaps the confusion matrix and performance across different flavors of toxicity.\n",
    "\n",
    "Credits:\n",
    "- https://www.kaggle.com/tanulsingh077/deep-learning-for-nlp-zero-to-transformers-bert\n",
    "- https://www.kaggle.com/jagangupta/stop-the-s-toxic-comments-eda\n",
    "- https://www.kaggle.com/clinma/eda-toxic-comment-classification-challenge\n",
    "- https://www.kaggle.com/abhi111/naive-bayes-baseline-and-logistic-regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from tqdm import tqdm \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "pd.options.display.max_rows = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toxicity import constants, data, features, text_preprocessing, model, metrics, visualize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_path = './../'+constants.INPUT_PATH\n",
    "df_train = data.load(input_data_path, filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xvalid, ytrain, yvalid = model.make_train_test(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers/Attention/BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Dependencies\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.metrics import Accuracy, AUC\n",
    "#from kaggle_datasets import KaggleDatasets\n",
    "import transformers\n",
    "\n",
    "from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using huggingface's tokenizer and DistilBert Model.\n",
    "https://huggingface.co/transformers/main_classes/tokenizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMP DATA FOR CONFIG\n",
    "#AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# Configuration\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 16 \n",
    "MAX_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_encode(texts, tokenizer, chunk_size=256, maxlen=MAX_LEN):\n",
    "    \"\"\"\n",
    "    Encoder for encoding the text into sequence of integers for BERT Input\n",
    "    \"\"\"\n",
    "    #Only a small fraction of input is > maxlen, not biased across toxic/nontoxic.\n",
    "    tokenizer.enable_truncation(max_length=maxlen)\n",
    "    tokenizer.enable_padding(length=maxlen)\n",
    "    all_ids = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), chunk_size)):\n",
    "        text_chunk = texts[i:i+chunk_size].tolist()\n",
    "        encs = tokenizer.encode_batch(text_chunk)\n",
    "        all_ids.extend([enc.ids for enc in encs])\n",
    "    \n",
    "    return np.array(all_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "For understanding please refer to hugging face documentation again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer(vocabulary_size=28996, model=BertWordPiece, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK], clean_text=True, handle_chinese_chars=True, strip_accents=None, lowercase=False, wordpieces_prefix=##)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First load the real tokenizer\n",
    "tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n",
    "# Save the loaded tokenizer locally\n",
    "tokenizer.save_pretrained('.')\n",
    "# Reload it with the huggingface tokenizers library\n",
    "fast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=False)\n",
    "fast_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 699/699 [00:26<00:00, 26.53it/s]\n",
      "100%|██████████| 175/175 [00:05<00:00, 30.82it/s]\n"
     ]
    }
   ],
   "source": [
    "x_train = fast_encode(xtrain.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n",
    "x_valid = fast_encode(xvalid.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n",
    "#x_test = fast_encode(test.content.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n",
    "\n",
    "y_train = ytrain\n",
    "y_valid = yvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(input, n=50):\n",
    "    return input[0:n]\n",
    "\n",
    "x_train_s = sample(x_train)\n",
    "x_valid_s = sample(x_valid)\n",
    "y_train_s = sample(y_train)\n",
    "y_valid_s = sample(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(transformer, max_len=MAX_LEN, transformer_trainable=False):\n",
    "    \"\"\"\n",
    "    function for training the BERT model\n",
    "    \"\"\"\n",
    "    transformer.trainable = transformer_trainable\n",
    "    input_word_ids = Input(shape=(max_len,), dtype='int32', name=\"input_word_ids\")\n",
    "    sequence_output = transformer(input_word_ids)[0]\n",
    "    cls_token = sequence_output[:, 0, :]\n",
    "    dense_1 = Dense(200, activation='relu')(cls_token)\n",
    "    out = Dense(1, activation='sigmoid')(dense_1)\n",
    "    \n",
    "    model = Model(inputs=input_word_ids, outputs=out)\n",
    "    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy', AUC(curve='PR')])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Training\n",
    "\n",
    "If you want to use any another model just replace the model name in transformers._____ and use accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "transformer_layer = (\n",
    "    transformers.TFDistilBertModel\n",
    "    .from_pretrained('distilbert-base-cased')\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainable vs. frozen variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(transformer_layer, max_len=MAX_LEN)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a callback for tensorboard\n",
    "tb_callback = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "# Create a callback that saves the model's weights every epoch\n",
    "checkpoint_path = \"training/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_path, \n",
    "    verbose=1, \n",
    "    save_weights_only=True,\n",
    "    save_freq='epoch')\n",
    "\n",
    "# Callback for early stopping if model isn't improving\n",
    "es = EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto',\n",
    "    baseline=None, restore_best_weights=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history = model.fit(\n",
    "    x_train_s,\n",
    "    y_train_s,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(x_valid_s, y_valid_s),\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[cp_callback, tb_callback, es]\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(\n",
    "    x_valid_s\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toxicity.metrics import run_metrics\n",
    "run_metrics(y_pred>.5, y_pred, y_valid_s, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore max sequence length for toxic and benign comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_comment_length(x, y):\n",
    "    texts=x.astype(str)\n",
    "    tokenizer=fast_tokenizer\n",
    "    chunk_size=256\n",
    "    all_ids = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), chunk_size)):\n",
    "        text_chunk = texts[i:i+chunk_size].tolist()\n",
    "        encs = tokenizer.encode_batch(text_chunk)\n",
    "        all_ids.extend([enc.ids for enc in encs])\n",
    "\n",
    "    lens = []\n",
    "    for j in range(len(all_ids)):\n",
    "        lens.append(len(all_ids[j]))\n",
    "\n",
    "    plt.hist(lens, 50)\n",
    "    plt.yscale('log')\n",
    "\n",
    "    long_index = (np.array(lens)>500)\n",
    "    long_index = (np.array(lens)>500)\n",
    "    print('Number of comments: ' + str(len(long_index)))\n",
    "    print('Number of toxic comments: ' + str(sum(y)))\n",
    "    print('Number of comments longer than 500 tokens: ' + str(sum(long_index)))\n",
    "    print('Number of toxic comments longer than 500 tokens: ' + str(sum(y[long_index])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 699/699 [00:28<00:00, 24.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of comments: 178839\n",
      "Number of toxic comments: 17107\n",
      "Number of comments longer than 500 tokens: 178839\n",
      "Number of toxic comments longer than 500 tokens: 17107\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "explore_comment_length(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175/175 [00:05<00:00, 29.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of comments: 44710\n",
      "Number of toxic comments: 4277\n",
      "Number of comments longer than 500 tokens: 44710\n",
      "Number of toxic comments longer than 500 tokens: 4277\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "explore_comment_length(xvalid, yvalid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer.encode_batch(['man walks down the street happily don''t you think @fire aslkfd291o'])\n",
    "\n",
    "print(encoded[0].ids)\n",
    "for id_item in encoded[0].ids:\n",
    "    print(tokenizer.id_to_token(id_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testcase\n",
    "fast_tokenizer.token_to_id('[UNK]')\n",
    "print(fast_tokenizer.token_to_id('Man'))\n",
    "print(fast_tokenizer.token_to_id('man'))\n",
    "fast_tokenizer.id_to_token(28995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
